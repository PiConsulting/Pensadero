{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Vamos a seleccionar características mediante una combinación de varianza, eliminación de duplicados y de correlación. Al final, compararemos el rendimiento de los modelos de aprendizaje automático creados con los diferentes subconjuntos de características.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 301)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\TomasCammisa\\Desktop\\Udemy\\Feature Selection for ML\\Precleaned datasets\\dataset_1.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 300), (15000, 300))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separamos en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo una copia del conjunto de datos con todas las variables\n",
    "# para medir el rendimiento de los modelos de aprendizaje automático\n",
    "\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removemos variables constantes\n",
    "\n",
    "Podemos hacer de dos maneras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 266), (15000, 266))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#La primera manera es calculando la desviación estandar de cada feature\n",
    "#Obtenemos las que tiene std = 0 y las eliminamos de nuestro dataset\n",
    "constant_features = [\n",
    "    feat for feat in X_train.columns if X_train[feat].std() == 0\n",
    "]\n",
    "\n",
    "X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El camino alternativo es la utilización de VarianceThreshold de scikit learn\n",
    "\n",
    "sel = VarianceThreshold(     #Definimos umbral minimo de varianza \n",
    "    threshold=0)  \n",
    "\n",
    "sel.fit(X_train)  # fit encuentra variables con varianza inferior al minimo\n",
    "\n",
    "sum(sel.get_support()) # Contamos cantidad de variables que superan el umbral minimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos variables con las que nos quedamos\n",
    "features_to_keep = X_train.columns[sel.get_support()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'var_6', 'var_7', 'var_8',\n",
       "       'var_9', 'var_10',\n",
       "       ...\n",
       "       'var_289', 'var_290', 'var_291', 'var_292', 'var_293', 'var_295',\n",
       "       'var_296', 'var_298', 'var_299', 'var_300'],\n",
       "      dtype='object', length=266)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 266), (15000, 266))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removemos las variables\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformarmos el array que obtenemos como resultado en un nuevo df con las variables \n",
    "#que superaron el umbral minimo de varianza\n",
    "\n",
    "X_train= pd.DataFrame(X_train)\n",
    "X_train.columns = features_to_keep\n",
    "\n",
    "X_test= pd.DataFrame(X_test)\n",
    "X_test.columns = features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35000, 266)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removemos variables cuasi constantes\n",
    "\n",
    "El minimo de varianza es subjetivo al problema que estemos resolviendo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminamos variables cuasi constantes\n",
    "sel = VarianceThreshold(\n",
    "    threshold=0.01)  \n",
    "\n",
    "sel.fit(X_train)  # fit encuentra variables con varianza inferior al minimo\n",
    "\n",
    "sum(sel.get_support()) # Contamos cantidad de variables que superan el umbral minimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos variables con las que nos quedamos\n",
    "features_to_keep = X_train.columns[sel.get_support()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_3', 'var_4', 'var_5', 'var_6', 'var_8', 'var_11', 'var_12',\n",
       "       'var_13', 'var_14', 'var_15',\n",
       "       ...\n",
       "       'var_286', 'var_288', 'var_290', 'var_291', 'var_292', 'var_293',\n",
       "       'var_295', 'var_296', 'var_299', 'var_300'],\n",
       "      dtype='object', length=215)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 215), (15000, 215))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removemos las variables\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformarmos el array que obtenemos como resultado en un nuevo df con las variables \n",
    "#que superaron el umbral minimo de varianza\n",
    "\n",
    "X_train= pd.DataFrame(X_train)\n",
    "X_train.columns = features_to_keep\n",
    "\n",
    "X_test= pd.DataFrame(X_test)\n",
    "X_test.columns = features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>...</th>\n",
       "      <th>var_286</th>\n",
       "      <th>var_288</th>\n",
       "      <th>var_290</th>\n",
       "      <th>var_291</th>\n",
       "      <th>var_292</th>\n",
       "      <th>var_293</th>\n",
       "      <th>var_295</th>\n",
       "      <th>var_296</th>\n",
       "      <th>var_299</th>\n",
       "      <th>var_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.79</td>\n",
       "      <td>85435.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   var_3  var_4    var_5  var_6  var_8  var_11  var_12  var_13  var_14  \\\n",
       "0    0.0   2.79      0.0    0.0    0.0     0.0     0.0     0.0     0.0   \n",
       "1    0.0   2.97      0.0    0.0    0.0     0.0     0.0     0.0     0.0   \n",
       "2    0.0   0.00      0.0    0.0    0.0     0.0     0.0     0.0     0.0   \n",
       "3    0.0   2.79  85435.2    0.0    0.0     0.0     0.0     0.0     0.0   \n",
       "4    0.0   5.70      0.0    0.0    0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   var_15  ...  var_286  var_288  var_290  var_291  var_292  var_293  var_295  \\\n",
       "0     3.0  ...      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1     3.0  ...      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2     0.0  ...      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3     0.0  ...      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4     3.0  ...      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   var_296  var_299  var_300  \n",
       "0      0.0      0.0      0.0  \n",
       "1      0.0      0.0      0.0  \n",
       "2      0.0      0.0      0.0  \n",
       "3      0.0      0.0      0.0  \n",
       "4      0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 215 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya eliminamos 75 variables de nuestro data set de 300, es decir, el 25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de contar con modulos de informacion distinta, donde las unidades de medida de cada feature son distintas es recomendable:\n",
    "1. Subdividir en modulos y aplicar el minimo de varianza a cada uno de ellos o\n",
    "2. Normalizar el dataset antes de aplicar el minimo de varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removemos features duplicadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datasets suelen features duplicadas, es decir, features que, a pesar de tener nombres diferentes, son idénticas. Muchas veces incluimos features duplicadas cuando realizamos una codificación de variables categóricas.\n",
    "\n",
    "No hay función en Pandas para encontrar columnas duplicadas,por lo que debemos crear un codigo para iterar sobre nuestra base de datos y eliminar las mismas.\n",
    "\n",
    "Advetencia: puede ser una operación computacionalmente costosa en Python, por lo tanto, dependiendo del tamaño de su conjunto de datos, es posible que no siempre pueda hacerlo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Codigo detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "# comprobar si hay features duplicadas en el dataset:\n",
    "\n",
    "# crear un diccionario donde almacenaremos\n",
    "# las features duplicadas\n",
    "duplicated_feat_pairs = {}\n",
    "\n",
    "\n",
    "# creamos una lista vacía para alamcenar las features\n",
    "# que se encontraron duplicados\n",
    "_duplicated_feat = []\n",
    "\n",
    "\n",
    "# iteramos sobre cada feature en nuestro dataset:\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    \n",
    "    # esta linea de codigo nos ayuda a entender dónde está el loop\n",
    "    if i % 10 == 0:  \n",
    "        print(i)\n",
    "    \n",
    "    # seleccionamos la feature 1:\n",
    "    feat_1 = X_train.columns[i]\n",
    "    \n",
    "    # verifica si esta feature ya ha sido identificada\n",
    "    # como un duplicado de otra. Si lo fuera, debe almacenarse en\n",
    "    # nuestra lista _duplicated_feat.\n",
    "    \n",
    "    # Si esta función ya se identificó como un duplicado, la omitimos, si\n",
    "    # aún no ha sido identificado como duplicado, entonces seguimos:\n",
    "    if feat_1 not in _duplicated_feat:\n",
    "    \n",
    "        # creamos una lista vacía como entrada para esta función en el diccionario:\n",
    "        duplicated_feat_pairs[feat_1] = []\n",
    "\n",
    "        # iteramos sobre el resto de las columnas del dataset:\n",
    "        for feat_2 in X_train.columns[i + 1:]:\n",
    "\n",
    "            # comprobamose si esta segunda feature es idéntica a la primera\n",
    "            if X_train[feat_1].equals(X_train[feat_2]):\n",
    "\n",
    "                # si es idéntica, la agregamos al diccionaro\n",
    "                duplicated_feat_pairs[feat_1].append(feat_2)\n",
    "                \n",
    "                # y la sumamos a nuestra lista de variables duplicadas\n",
    "                _duplicated_feat.append(feat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificamos extension de nuestra lista de variables duplicadas\n",
    "len(_duplicated_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['var_151',\n",
       " 'var_183',\n",
       " 'var_148',\n",
       " 'var_216',\n",
       " 'var_199',\n",
       " 'var_296',\n",
       " 'var_239',\n",
       " 'var_263',\n",
       " 'var_232',\n",
       " 'var_269']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estas variables son:\n",
    "_duplicated_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'var_3': [],\n",
       " 'var_4': [],\n",
       " 'var_5': [],\n",
       " 'var_6': ['var_151'],\n",
       " 'var_8': [],\n",
       " 'var_11': [],\n",
       " 'var_12': [],\n",
       " 'var_13': [],\n",
       " 'var_14': [],\n",
       " 'var_15': [],\n",
       " 'var_16': [],\n",
       " 'var_17': [],\n",
       " 'var_18': [],\n",
       " 'var_20': [],\n",
       " 'var_21': [],\n",
       " 'var_22': [],\n",
       " 'var_24': [],\n",
       " 'var_25': [],\n",
       " 'var_26': [],\n",
       " 'var_27': [],\n",
       " 'var_29': [],\n",
       " 'var_30': [],\n",
       " 'var_31': [],\n",
       " 'var_32': [],\n",
       " 'var_34': ['var_183'],\n",
       " 'var_35': [],\n",
       " 'var_37': ['var_148'],\n",
       " 'var_38': [],\n",
       " 'var_39': [],\n",
       " 'var_40': [],\n",
       " 'var_41': [],\n",
       " 'var_42': [],\n",
       " 'var_46': [],\n",
       " 'var_47': [],\n",
       " 'var_48': [],\n",
       " 'var_49': [],\n",
       " 'var_50': [],\n",
       " 'var_51': [],\n",
       " 'var_52': [],\n",
       " 'var_54': [],\n",
       " 'var_55': [],\n",
       " 'var_57': [],\n",
       " 'var_58': [],\n",
       " 'var_60': ['var_216'],\n",
       " 'var_62': [],\n",
       " 'var_63': [],\n",
       " 'var_64': [],\n",
       " 'var_65': [],\n",
       " 'var_68': [],\n",
       " 'var_70': [],\n",
       " 'var_72': [],\n",
       " 'var_73': [],\n",
       " 'var_74': [],\n",
       " 'var_75': [],\n",
       " 'var_76': [],\n",
       " 'var_77': [],\n",
       " 'var_78': [],\n",
       " 'var_79': [],\n",
       " 'var_82': [],\n",
       " 'var_83': [],\n",
       " 'var_84': ['var_199'],\n",
       " 'var_85': [],\n",
       " 'var_86': [],\n",
       " 'var_88': [],\n",
       " 'var_90': [],\n",
       " 'var_91': [],\n",
       " 'var_93': [],\n",
       " 'var_94': [],\n",
       " 'var_95': [],\n",
       " 'var_96': [],\n",
       " 'var_98': [],\n",
       " 'var_100': [],\n",
       " 'var_101': [],\n",
       " 'var_102': [],\n",
       " 'var_103': [],\n",
       " 'var_105': [],\n",
       " 'var_107': [],\n",
       " 'var_108': [],\n",
       " 'var_109': [],\n",
       " 'var_110': [],\n",
       " 'var_111': [],\n",
       " 'var_114': [],\n",
       " 'var_115': [],\n",
       " 'var_117': [],\n",
       " 'var_118': [],\n",
       " 'var_119': [],\n",
       " 'var_121': [],\n",
       " 'var_123': [],\n",
       " 'var_124': [],\n",
       " 'var_125': [],\n",
       " 'var_126': [],\n",
       " 'var_128': [],\n",
       " 'var_129': [],\n",
       " 'var_130': [],\n",
       " 'var_131': [],\n",
       " 'var_132': [],\n",
       " 'var_134': [],\n",
       " 'var_136': [],\n",
       " 'var_138': [],\n",
       " 'var_139': [],\n",
       " 'var_140': [],\n",
       " 'var_142': [],\n",
       " 'var_143': ['var_296'],\n",
       " 'var_144': [],\n",
       " 'var_145': [],\n",
       " 'var_147': [],\n",
       " 'var_149': ['var_239'],\n",
       " 'var_150': [],\n",
       " 'var_152': [],\n",
       " 'var_153': [],\n",
       " 'var_154': [],\n",
       " 'var_155': [],\n",
       " 'var_156': [],\n",
       " 'var_157': [],\n",
       " 'var_159': [],\n",
       " 'var_160': [],\n",
       " 'var_161': [],\n",
       " 'var_162': [],\n",
       " 'var_163': [],\n",
       " 'var_164': [],\n",
       " 'var_165': [],\n",
       " 'var_166': [],\n",
       " 'var_168': [],\n",
       " 'var_169': [],\n",
       " 'var_172': [],\n",
       " 'var_173': [],\n",
       " 'var_174': [],\n",
       " 'var_175': [],\n",
       " 'var_176': [],\n",
       " 'var_179': [],\n",
       " 'var_181': [],\n",
       " 'var_184': [],\n",
       " 'var_185': [],\n",
       " 'var_186': [],\n",
       " 'var_188': [],\n",
       " 'var_190': [],\n",
       " 'var_191': [],\n",
       " 'var_192': [],\n",
       " 'var_193': [],\n",
       " 'var_200': [],\n",
       " 'var_203': [],\n",
       " 'var_204': [],\n",
       " 'var_205': [],\n",
       " 'var_206': [],\n",
       " 'var_207': [],\n",
       " 'var_208': [],\n",
       " 'var_209': [],\n",
       " 'var_210': [],\n",
       " 'var_211': [],\n",
       " 'var_213': [],\n",
       " 'var_214': [],\n",
       " 'var_217': [],\n",
       " 'var_220': [],\n",
       " 'var_221': ['var_263'],\n",
       " 'var_222': [],\n",
       " 'var_224': [],\n",
       " 'var_226': ['var_232'],\n",
       " 'var_228': [],\n",
       " 'var_229': ['var_269'],\n",
       " 'var_230': [],\n",
       " 'var_231': [],\n",
       " 'var_236': [],\n",
       " 'var_237': [],\n",
       " 'var_238': [],\n",
       " 'var_240': [],\n",
       " 'var_241': [],\n",
       " 'var_242': [],\n",
       " 'var_243': [],\n",
       " 'var_244': [],\n",
       " 'var_246': [],\n",
       " 'var_252': [],\n",
       " 'var_253': [],\n",
       " 'var_254': [],\n",
       " 'var_255': [],\n",
       " 'var_257': [],\n",
       " 'var_258': [],\n",
       " 'var_259': [],\n",
       " 'var_261': [],\n",
       " 'var_262': [],\n",
       " 'var_264': [],\n",
       " 'var_265': [],\n",
       " 'var_266': [],\n",
       " 'var_268': [],\n",
       " 'var_270': [],\n",
       " 'var_271': [],\n",
       " 'var_272': [],\n",
       " 'var_273': [],\n",
       " 'var_275': [],\n",
       " 'var_276': [],\n",
       " 'var_277': [],\n",
       " 'var_278': [],\n",
       " 'var_279': [],\n",
       " 'var_280': [],\n",
       " 'var_281': [],\n",
       " 'var_283': [],\n",
       " 'var_284': [],\n",
       " 'var_286': [],\n",
       " 'var_288': [],\n",
       " 'var_290': [],\n",
       " 'var_291': [],\n",
       " 'var_292': [],\n",
       " 'var_293': [],\n",
       " 'var_295': [],\n",
       " 'var_299': [],\n",
       " 'var_300': []}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos el diccionario de pares de features duplicadas\n",
    "# Si, por cada variable, tiene una feature en la lista, quiere decir que esta feature esta duplicada.\n",
    "duplicated_feat_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_6 ['var_151']\n",
      "\n",
      "var_34 ['var_183']\n",
      "\n",
      "var_37 ['var_148']\n",
      "\n",
      "var_60 ['var_216']\n",
      "\n",
      "var_84 ['var_199']\n",
      "\n",
      "var_143 ['var_296']\n",
      "\n",
      "var_149 ['var_239']\n",
      "\n",
      "var_221 ['var_263']\n",
      "\n",
      "var_226 ['var_232']\n",
      "\n",
      "var_229 ['var_269']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Veamos solo las features duplicadas\n",
    "\n",
    "# iteramos sobre cada variable en el diccionario:\n",
    "for feat in duplicated_feat_pairs.keys():\n",
    "    \n",
    "    # si tiene duplicados, la lista no debe estar vacía::\n",
    "    if len(duplicated_feat_pairs[feat]) > 0:\n",
    "\n",
    "        # hacemos print de la varaible y su duplicado:\n",
    "        print(feat, duplicated_feat_pairs[feat])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_151</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   var_6  var_151\n",
       "0    0.0      0.0\n",
       "1    0.0      0.0\n",
       "2    0.0      0.0\n",
       "3    0.0      0.0\n",
       "4    0.0      0.0\n",
       "5    0.0      0.0\n",
       "6    0.0      0.0\n",
       "7    0.0      0.0\n",
       "8    0.0      0.0\n",
       "9    0.0      0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seleccionemos un par de features para ver de que se trata\n",
    "\n",
    "X_train[['var_6', 'var_151']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Codigo simplificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buscamos variables dupplicadas en nuestro dataset\n",
    "duplicated_feat = []  #Creamos una lista donde alamacenaremos las variables duplicadas\n",
    "for i in range(0, len(X_train.columns)): #iteramos sobre nuestro df\n",
    "    if i % 10 == 0:  \n",
    "        print(i)\n",
    "\n",
    "    col_1 = X_train.columns[i] #Seleccionamos la primer feature\n",
    "\n",
    "    for col_2 in X_train.columns[i + 1:]: \n",
    "        if X_train[col_1].equals(X_train[col_2]):\n",
    "            duplicated_feat.append(col_2)\n",
    "            \n",
    "len(duplicated_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 205), (15000, 205))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removemos duplicados\n",
    "X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos una copia del df\n",
    "# para medir el rendimiento de los modelos\n",
    "# al final\n",
    "\n",
    "X_train_basic_filter = X_train.copy()\n",
    "X_test_basic_filter = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminamos variables correlacionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Porque eliminar variables correlacionadas?\n",
    "\"Los buenos subconjuntos de datasets contienen variables altamente correlacionadas con la variable objetivo, pero no correlacionadas entre sí\". (Cammisa, 2022, p.271)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9901870592788291 var_14 var_13\n",
      "0.9747684936252511 var_40 var_16\n",
      "0.9019247145301009 var_54 var_51\n",
      "0.8080394007343367 var_55 var_21\n",
      "0.9469585731168426 var_62 var_31\n",
      "0.9871515157796921 var_64 var_47\n",
      "0.8790256734547003 var_65 var_3\n",
      "1.0000000000000098 var_73 var_34\n",
      "0.9215085514013303 var_86 var_30\n",
      "0.9996086272168176 var_88 var_38\n",
      "0.840948382976813 var_93 var_41\n",
      "0.992979818656033 var_94 var_84\n",
      "0.999171276440353 var_100 var_96\n",
      "0.9364005277628559 var_101 var_18\n",
      "0.8473817067229984 var_102 var_16\n",
      "0.884479402585018 var_102 var_40\n",
      "0.8906887043129583 var_103 var_52\n",
      "0.8842323791033176 var_105 var_91\n",
      "0.8919991045566614 var_107 var_58\n",
      "0.9798048008794173 var_114 var_58\n",
      "0.8691881944783044 var_114 var_107\n",
      "0.9061739769575459 var_117 var_38\n",
      "0.8017092998502472 var_117 var_58\n",
      "0.9065287676443693 var_117 var_88\n",
      "0.8039746471057304 var_117 var_114\n",
      "0.9294747773002267 var_121 var_83\n",
      "0.9048991262430333 var_123 var_52\n",
      "0.9864956619134007 var_123 var_103\n",
      "0.9548493309404984 var_125 var_98\n",
      "0.9901870592788358 var_129 var_13\n",
      "1.0000000000000024 var_129 var_14\n",
      "0.9684286480580293 var_130 var_16\n",
      "0.9441116120389779 var_130 var_40\n",
      "0.9027572780082797 var_132 var_15\n",
      "0.8478467513972774 var_139 var_26\n",
      "0.8845855663834701 var_140 var_118\n",
      "0.9838870620305379 var_143 var_37\n",
      "0.8106525795662803 var_144 var_140\n",
      "0.9106904156858897 var_145 var_50\n",
      "0.8415659612759214 var_152 var_15\n",
      "0.8661221290773906 var_152 var_132\n",
      "1.0000000000000036 var_153 var_72\n",
      "0.8760333462404475 var_155 var_38\n",
      "0.8764545601674404 var_155 var_88\n",
      "0.9534048305278597 var_155 var_117\n",
      "0.8026839723472257 var_156 var_26\n",
      "0.8280903195935249 var_160 var_52\n",
      "0.963129451314554 var_160 var_103\n",
      "0.9501230255828543 var_160 var_123\n",
      "0.8786305362142801 var_161 var_50\n",
      "0.9834525351767698 var_162 var_52\n",
      "0.9021077072522089 var_162 var_103\n",
      "0.8899253397829455 var_162 var_123\n",
      "0.8376530676639514 var_162 var_160\n",
      "0.8259494831082841 var_163 var_52\n",
      "0.9409175617472093 var_163 var_103\n",
      "0.9523427082236874 var_163 var_123\n",
      "0.9777063456481765 var_163 var_160\n",
      "0.8082841551589949 var_163 var_162\n",
      "0.9032529912322542 var_164 var_83\n",
      "0.9717885985630288 var_164 var_121\n",
      "0.9450840523440127 var_165 var_27\n",
      "0.9902322735909377 var_166 var_91\n",
      "0.8927973211148599 var_166 var_105\n",
      "0.8355870882570865 var_168 var_37\n",
      "0.8224482327868884 var_168 var_143\n",
      "0.9777805949032589 var_172 var_51\n",
      "0.899953357684289 var_172 var_54\n",
      "0.8910827797638777 var_175 var_31\n",
      "0.8747574042803871 var_175 var_62\n",
      "0.8065566451903773 var_175 var_74\n",
      "0.8092134836078299 var_176 var_70\n",
      "0.9182435860715703 var_176 var_169\n",
      "0.9004499193374975 var_179 var_31\n",
      "0.8660494771321747 var_179 var_62\n",
      "0.8019758794022095 var_179 var_74\n",
      "0.9895091669290453 var_179 var_175\n",
      "0.8821147729318506 var_184 var_95\n",
      "0.9621135744096729 var_186 var_41\n",
      "0.882131465978123 var_186 var_93\n",
      "0.9967896477938974 var_190 var_50\n",
      "0.9120711684883829 var_190 var_145\n",
      "0.8775672276471755 var_190 var_161\n",
      "0.966086802157502 var_191 var_108\n",
      "0.818780468187515 var_192 var_93\n",
      "0.9175211461938486 var_192 var_118\n",
      "0.9462890482346373 var_192 var_140\n",
      "0.99895502466863 var_203 var_57\n",
      "0.8668369097388188 var_207 var_118\n",
      "0.909344173669558 var_207 var_140\n",
      "0.8668333859966909 var_207 var_144\n",
      "0.8885112270706096 var_207 var_192\n",
      "0.9261357267557845 var_209 var_70\n",
      "1.0000000000000016 var_210 var_12\n",
      "0.9288237249756828 var_213 var_27\n",
      "0.8033230821243764 var_213 var_154\n",
      "0.9880883807966548 var_213 var_165\n",
      "0.9264510884402554 var_214 var_96\n",
      "0.9247655814383736 var_214 var_100\n",
      "1.0000000000000084 var_217 var_72\n",
      "0.9999999999999971 var_217 var_153\n",
      "0.9679152612076674 var_222 var_30\n",
      "0.8792371575567984 var_222 var_86\n",
      "0.8932030004130473 var_226 var_83\n",
      "0.830209659892797 var_226 var_121\n",
      "0.8067882819007034 var_226 var_164\n",
      "0.8694152146881413 var_229 var_37\n",
      "0.8879359748446397 var_229 var_143\n",
      "0.9254214804571187 var_229 var_168\n",
      "0.8719263451379741 var_230 var_152\n",
      "0.8385116596208573 var_237 var_16\n",
      "0.8671610359247669 var_237 var_40\n",
      "0.9960845273052544 var_237 var_102\n",
      "0.8651265932119202 var_241 var_70\n",
      "0.8709643480147017 var_241 var_169\n",
      "0.9359036922267325 var_241 var_176\n",
      "0.904955342420244 var_244 var_220\n",
      "0.9660664246191228 var_252 var_83\n",
      "0.9016552170484233 var_252 var_121\n",
      "0.9349802127616365 var_252 var_164\n",
      "0.862893429068104 var_252 var_226\n",
      "0.983033678856734 var_253 var_30\n",
      "0.9181798645812691 var_253 var_86\n",
      "0.9527688284904495 var_253 var_222\n",
      "0.9988349923656441 var_254 var_77\n",
      "0.8848354860669871 var_255 var_91\n",
      "0.9986270645229104 var_255 var_105\n",
      "0.8922554563144441 var_255 var_166\n",
      "0.9999999999999865 var_257 var_136\n",
      "0.983452535176768 var_258 var_52\n",
      "0.902107707252198 var_258 var_103\n",
      "0.8899253397829417 var_258 var_123\n",
      "0.8376530676639399 var_258 var_160\n",
      "0.9999999999999963 var_258 var_162\n",
      "0.8082841551589963 var_258 var_163\n",
      "0.8988041554874652 var_259 var_38\n",
      "0.8091168922730672 var_259 var_58\n",
      "0.8984646291167394 var_259 var_88\n",
      "0.9899983650779778 var_259 var_117\n",
      "0.9440085382129914 var_259 var_155\n",
      "0.8062493475820022 var_261 var_140\n",
      "0.9926511351051601 var_261 var_144\n",
      "0.8766463770482004 var_261 var_207\n",
      "0.9808879484712734 var_262 var_4\n",
      "0.8414992839586677 var_264 var_60\n",
      "0.885962213400796 var_270 var_37\n",
      "0.8721272489279259 var_270 var_143\n",
      "0.9431407746495857 var_270 var_168\n",
      "0.9813231326772632 var_270 var_229\n",
      "0.9983061095043411 var_272 var_220\n",
      "0.9033752144431474 var_272 var_244\n",
      "0.8694303392449065 var_273 var_159\n",
      "0.8271648862959567 var_273 var_240\n",
      "0.9931124131190434 var_275 var_174\n",
      "0.963412263882593 var_277 var_118\n",
      "0.9175628091909714 var_277 var_140\n",
      "0.870216827711703 var_277 var_192\n",
      "0.8711753111467 var_277 var_207\n",
      "0.866699920865136 var_278 var_238\n",
      "0.9093062312629782 var_279 var_118\n",
      "0.8550988665835179 var_279 var_140\n",
      "0.8111491507165989 var_279 var_192\n",
      "0.9227062155790074 var_279 var_207\n",
      "0.9421493701657989 var_279 var_277\n",
      "1.0000000000000049 var_280 var_12\n",
      "1.0000000000000013 var_280 var_210\n",
      "0.9988276356996896 var_281 var_8\n",
      "0.9999999999999993 var_283 var_12\n",
      "0.9999999999999887 var_283 var_210\n",
      "0.999999999999989 var_283 var_280\n",
      "0.9399566343298983 var_284 var_38\n",
      "0.940324651805974 var_284 var_88\n",
      "0.8521936260384676 var_284 var_117\n",
      "0.8276646865907551 var_284 var_155\n",
      "0.8446094691983694 var_284 var_259\n",
      "0.9736653298007307 var_286 var_39\n",
      "0.9165843793445126 var_290 var_78\n",
      "0.9364880132531701 var_291 var_236\n",
      "0.9397169393845912 var_295 var_38\n",
      "0.9393488694248067 var_295 var_88\n",
      "0.8513100167436827 var_295 var_117\n",
      "0.8262110308782834 var_295 var_155\n",
      "0.8443580357481709 var_295 var_259\n",
      "0.9986121695775954 var_295 var_284\n",
      "0.992610778557984 var_299 var_78\n",
      "0.8778687153334389 var_299 var_290\n",
      "0.9851241830441231 var_300 var_30\n",
      "0.8952113513093993 var_300 var_86\n",
      "0.9770780847924525 var_300 var_222\n",
      "0.9708196103266921 var_300 var_253\n",
      "correlated features:  93\n"
     ]
    }
   ],
   "source": [
    "# con la siguiente función podemos seleccionar features altamente correlacionadas\n",
    "# eliminará la primera feature que se correlaciona con cualquiera otra\n",
    "# sin más información.\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # creamos un set donde guardamos los nombres de las columnas correlacionadas\n",
    "    \n",
    "    corr_matrix = dataset.corr() #creamos la matriz de correlacion\n",
    "    \n",
    "    # para cada feature en el dataset (columnas de la matriz de correlación)\n",
    "    for i in range(len(corr_matrix.columns)):  \n",
    "    \n",
    "        for j in range(i):  #verificamos con otras features\n",
    "            \n",
    "            # si la correlación es superior a un cierto umbral\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                \n",
    "                #linea opcional para ver correlacion entre cada par de variables con corr mayor al umbral minimo\n",
    "                print(abs(corr_matrix.iloc[i, j]), corr_matrix.columns[i], corr_matrix.columns[j]) \n",
    "                \n",
    "                colname = corr_matrix.columns[i]  # obtenemos el nombre de las columnas correlacionadas\n",
    "                col_corr.add(colname)  #lo agregamos a nuestro set de variables correlacionadas\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8) #llamamos a la funcion\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  93\n"
     ]
    }
   ],
   "source": [
    "#Codigo simplificado\n",
    "# con la siguiente función podemos seleccionar features altamente correlacionadas\n",
    "# eliminará la primera feature que se correlaciona con cualquiera otra\n",
    "# sin más información.\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # creamos un set donde guardamos los nombres de las columnas correlacionadas\n",
    "    \n",
    "    corr_matrix = dataset.corr() #creamos la matriz de correlacion\n",
    "    \n",
    "    # para cada feature en el dataset (columnas de la matriz de correlación)\n",
    "    for i in range(len(corr_matrix.columns)):  \n",
    "    \n",
    "        for j in range(i):  #verificamos con otras features\n",
    "            \n",
    "            # si la correlación es superior a un cierto umbral\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                \n",
    "                #linea opcional para ver correlacion entre cada par de variables con corr mayor al umbral minimo\n",
    "                #print(abs(corr_matrix.iloc[i, j]), corr_matrix.columns[i], corr_matrix.columns[j]) \n",
    "                \n",
    "                colname = corr_matrix.columns[i]  # obtenemos el nombre de las columnas correlacionadas\n",
    "                col_corr.add(colname)  #lo agregamos a nuestro set de variables correlacionadas\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8) #llamamos a la funcion\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 112), (15000, 112))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparamos performance de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear una función para random frest y comparar el rendimiento en el conjunto de entrenamiento y prueba\n",
    "\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.807612232524249\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7868832427636059\n"
     ]
    }
   ],
   "source": [
    "# df original con todas las variables\n",
    "run_randomForests(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.810290026780428\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7914020645941601\n"
     ]
    }
   ],
   "source": [
    "# filter methods - Basico - VARIANZA Y VARIABLES DUPLICADAS\n",
    "run_randomForests(X_train_basic_filter,\n",
    "                  X_test_basic_filter,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8066004772684517\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7859521124929707\n"
     ]
    }
   ],
   "source": [
    "# filter methods - Correlación\n",
    "run_randomForests(X_train,\n",
    "                  X_test,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La eliminación de variables constantes, casi constantes, duplicadas y correlacionadas redujo la cantidad de features drasticamente (de 300 a 112), sin afectar el rendimiento de los distintos modelos (0,786 frente a 0,7859)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una función para construir una regresión logística y comparar el rendimiento\n",
    "\n",
    "def run_logistic(X_train, X_test, y_train, y_test):\n",
    "    # función para entrenar y probar el rendimiento de la regresión logística\n",
    "    logit = LogisticRegression(random_state=44, max_iter=500)\n",
    "    logit.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = logit.predict_proba(X_train)\n",
    "    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    pred = logit.predict_proba(X_test)\n",
    "    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.8028231106726094\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.7950984398269426\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "# Primero escalamos los datos\n",
    "\n",
    "# original\n",
    "scaler = StandardScaler().fit(X_train_original)\n",
    "\n",
    "run_logistic(scaler.transform(X_train_original),\n",
    "             scaler.transform(X_test_original), y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.8022733407131674\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.7947416996589153\n"
     ]
    }
   ],
   "source": [
    "# filter methods - Basico\n",
    "scaler = StandardScaler().fit(X_train_basic_filter)\n",
    "\n",
    "run_logistic(scaler.transform(X_train_basic_filter),\n",
    "             scaler.transform(X_test_basic_filter),\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.7942679586909129\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.7881800430100233\n"
     ]
    }
   ],
   "source": [
    "# filter methods - Correlación\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "run_logistic(scaler.transform(X_train),\n",
    "             scaler.transform(X_test),\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera similar, para la regresión logística, la eliminación de características constantes, casi constantes, duplicadas y altamente correlacionadas no afectó drásticamente el rendimiento del algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea\n",
    "1. Probar distintos umbrales de varianza minima y ver como afecta a los resultados\n",
    "2. Probar con distintos umbrales de correlacion y ver como afecta a los modelos\n",
    "2. Probar con distinto set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.6px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
